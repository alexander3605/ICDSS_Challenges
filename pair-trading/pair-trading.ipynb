{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "\n",
    "from reader import get_prices\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.losses import MAPE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pprint\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT AND CLEAN DATASET\n",
    "\n",
    "prices = get_prices()\n",
    "symbols = prices.columns\n",
    "\n",
    "for stock in prices:\n",
    "    nan_dirty = prices[stock].isnull().sum().sum()\n",
    "    if nan_dirty > 2:\n",
    "        prices = prices.drop(stock, axis=1)\n",
    "    elif nan_dirty:\n",
    "        prices[stock].fillna(prices[stock].mean())\n",
    "symbols = prices.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER COMPANIES TOGETHER BASED ON STOCK TREND\n",
    "# FEATURE VECTOR USED: 5-DAY PERCENTAGE CHANGE\n",
    "\n",
    "weekly_priceDiff = {}\n",
    "\n",
    "symbol_count = 0\n",
    "for symbol in symbols:\n",
    "    stock = prices[symbol]\n",
    "    step = 5\n",
    "    temp_ls = []\n",
    "\n",
    "    for a in range(0,len(stock),step):\n",
    "        stock_diff = 0\n",
    "        opening_price = stock[a]\n",
    "        closing_price = None\n",
    "        try:\n",
    "            closing_price = stock[a+step-1]\n",
    "        except:\n",
    "            closing_price = stock[a+step-2]\n",
    "        stock_diff = (closing_price - opening_price)/opening_price*100\n",
    "        temp_ls.append(stock_diff)\n",
    "        \n",
    "    weekly_priceDiff[symbol] = temp_ls\n",
    "       \n",
    "\n",
    "weekly_priceDiff = pd.DataFrame(weekly_priceDiff)\n",
    "weekly_priceDiff = weekly_priceDiff.fillna(0)\n",
    "weekly_priceDiff = weekly_priceDiff.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_priceDiff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_no = 200\n",
    "\n",
    "kmeans = KMeans(n_clusters=clusters_no, random_state=2).fit(weekly_priceDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(kmeans.labels_)\n",
    "# print(kmeans.labels_)\n",
    "\n",
    "centers = np.zeros(clusters_no)\n",
    "for i in range(clusters_no):\n",
    "    centers[kmeans.labels_[i]] += 1\n",
    "    \n",
    "# print(centers)\n",
    "clusters = {}\n",
    "for cluster in range(clusters_no):\n",
    "    clusters[cluster] = symbols[np.where(kmeans.labels_==cluster)]\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans.predict([weekly_priceDiff.transpose().AAPL])\n",
    "\n",
    "\n",
    "# AAPL_cluster = kmeans.predict([weekly_priceDiff.transpose().AAPL])\n",
    "# for symbol in symbols:\n",
    "#     if kmeans.predict([weekly_priceDiff.transpose()[symbol]]) == AAPL_cluster:\n",
    "#         print(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANK CORRELATIONS IN SAME CLUSTER\n",
    "\n",
    "corr_A = []\n",
    "corr_B = []\n",
    "corr_coeff = []\n",
    "for c in clusters:\n",
    "    symb = clusters[c]\n",
    "    if len(symb) > 1:\n",
    "        values = []\n",
    "        for s in symb:\n",
    "            values.append(prices[s].fillna(prices[s].mean()).to_list()) # TO CHECK! filling NaN with mean\n",
    "        cluster_corr = np.corrcoef(values)\n",
    "        for i in range(len(symb)):\n",
    "            for j in range(i+1,len(symb)):\n",
    "                corr_A.append(symb[i])\n",
    "                corr_B.append(symb[j])\n",
    "                corr_coeff.append(cluster_corr[i][j])\n",
    "\n",
    "\n",
    "d = {}\n",
    "d[\"stock_A\"] = corr_A\n",
    "d[\"stock_B\"] = corr_B\n",
    "d[\"corr\"] = corr_coeff\n",
    "correlations = pd.DataFrame(data=d)\n",
    "\n",
    "correlations = correlations.sort_values([\"corr\"], ascending=[0])\n",
    "correlations = correlations.reset_index()\n",
    "correlations = correlations.drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Candidate pairs found: {len(correlations)}\")\n",
    "plt.plot(correlations[\"corr\"].to_list())\n",
    "\n",
    "correlation_bins = (0,0,0,0,0)\n",
    "bin0 = 0\n",
    "bin1 = 0\n",
    "bin2 = 0\n",
    "bin3 = 0\n",
    "bin4 = 0\n",
    "bin5 = 0\n",
    "for i in range(len(correlations)):\n",
    "    corr = correlations.loc[i,\"corr\"]\n",
    "\n",
    "    if corr >= 0.90:\n",
    "        bin0+=1\n",
    "    elif corr >= 0.75:\n",
    "        bin1 += 1\n",
    "    elif corr >= 0.50:\n",
    "        bin2 += 1\n",
    "    elif corr > -0.50:\n",
    "        bin3 += 1\n",
    "    elif corr > -0.75:\n",
    "        bin4 += 1\n",
    "    else:\n",
    "        bin5 += 1\n",
    "        \n",
    "print(f\" 1.00 -  0.90 Correlation pairs\\t{bin0}\\t({round(bin0/len(correlations)*100, 2)}%)\")\n",
    "print(f\" 0.90 -  0.75 Correlation pairs\\t{bin1}\\t({round(bin1/len(correlations)*100, 2)}%)\")\n",
    "print(f\" 0.75 -  0.50 Correlation pairs\\t{bin2}\\t({round(bin2/len(correlations)*100, 2)}%)\")\n",
    "print(f\" 0.50 - -0.50 Correlation pairs\\t{bin3}\\t({round(bin3/len(correlations)*100, 2)}%)\")\n",
    "print(f\"-0.50 - -0.75 Correlation pairs\\t{bin4}\\t({round(bin4/len(correlations)*100, 2)}%)\")\n",
    "print(f\"-0.75 - -1.00 Correlation pairs\\t{bin5}\\t({round(bin5/len(correlations)*100, 2)}%)\")\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_corr = correlations.loc[6,:]\n",
    "\n",
    "# REGULAR PLOT\n",
    "plt.plot(prices[best_corr.stock_A], label=best_corr.stock_A)\n",
    "plt.plot(prices[best_corr.stock_B], label=best_corr.stock_B)\n",
    "plt.legend()\n",
    "\n",
    "# SCALED PLOT\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# plt.plot(scaler.fit_transform(prices[best_corr.stock_A].to_numpy().reshape(-1,1)), label=best_corr.stock_A)\n",
    "# plt.plot(scaler.fit_transform(prices[best_corr.stock_B].to_numpy().reshape(-1,1)), label=best_corr.stock_B)\n",
    "# plt.legend()\n",
    "\n",
    "best_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain model only for stock pairs w/ correlation above treshold\n",
    "CORR_TRESHOLD = 0.95\n",
    "filtered_pairs = pd.DataFrame([pair for _,pair in correlations.iterrows() if pair['corr'] >= CORR_TRESHOLD])\n",
    "print(f\"Filtered {len(filtered_pairs)} stock pairs with correlation above {CORR_TRESHOLD}\")\n",
    "filtered_pairs = filtered_pairs[6:7]\n",
    "filtered_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION\n",
    "# convert an array of values into a dataset matrix\n",
    "look_back = 100\n",
    "def xy_split_dataset(dataset, look_back=1, normalize=True):\n",
    "    dataset = dataset.reshape(-1, 1)\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     if normalize:\n",
    "#         dataset = scaler.fit_transform(dataset) \n",
    "    \n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    X = np.array(dataX)\n",
    "    Y = np.array(dataY)\n",
    "    \n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PREPARE DATASET FOR TRAINING AND TESTING\n",
    "\n",
    "# dataset x-shape: (n_pairs, n_samples, look_back, 2)\n",
    "#         y-shape: (n_pairs, n_samples, 2)\n",
    "\n",
    "n_pairs = len(filtered_pairs)\n",
    "n_samples = len(prices.loc[:,best_corr.stock_B].to_numpy()) - look_back\n",
    "n_timesteps = look_back\n",
    "n_features = 2\n",
    "\n",
    "def pandas_fill(arr):\n",
    "    input_shape = arr.shape\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.fillna(method='ffill', axis=0, inplace=True)\n",
    "    out = df.to_numpy().reshape(input_shape)\n",
    "    return out\n",
    "\n",
    "print(\"Rearranging dataset...\")\n",
    "x_train = np.zeros(shape=(n_pairs, n_samples, look_back, 2), dtype=float)\n",
    "y_train = np.zeros(shape=(n_pairs, n_samples, 2), dtype=float)\n",
    "scalers = np.empty(shape=(n_pairs), dtype=MinMaxScaler)\n",
    "printer = True\n",
    "pair_idx = 0\n",
    "for _,pair in filtered_pairs.iterrows():\n",
    "    print(\"Pair {} of {}\".format(pair_idx+1, len(filtered_pairs)), end=\"\\r\")\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    stock_A = prices.loc[:,pair.stock_A].to_numpy()\n",
    "    stock_B = prices.loc[:,pair.stock_B].to_numpy()\n",
    "    \n",
    "    stock_A = pandas_fill(stock_A)\n",
    "    stock_B = pandas_fill(stock_B)\n",
    "    \n",
    "#     [stock_A, stock_B] = scaler.fit_transform([stock_A, stock_B])  \n",
    "    stock_A = np.log(stock_A)\n",
    "    stock_B = np.log(stock_B)\n",
    "    stock_A = pandas_fill(stock_A)\n",
    "    stock_B = pandas_fill(stock_B)\n",
    "    if np.sum(np.isnan(stock_A)):\n",
    "        print(\"found NAN - A\")\n",
    "    if np.sum(np.isnan(stock_B)):\n",
    "        print(\"found NAN - B\")\n",
    "    x_stock_A, y_stock_A = xy_split_dataset(stock_A, \n",
    "                                            look_back=look_back)\n",
    "    x_stock_B, y_stock_B = xy_split_dataset(stock_B, \n",
    "                                            look_back=look_back)\n",
    "    \n",
    "    for sample_idx in range(len(x_stock_A)):\n",
    "        for ts_idx in range(len(x_stock_A[sample_idx])):\n",
    "            x_train[pair_idx][sample_idx][ts_idx][0] = x_stock_A[sample_idx][ts_idx]\n",
    "            x_train[pair_idx][sample_idx][ts_idx][1] = x_stock_B[sample_idx][ts_idx]\n",
    "        y_train[pair_idx][sample_idx][0] = y_stock_A[sample_idx]\n",
    "        y_train[pair_idx][sample_idx][1] = y_stock_B[sample_idx]\n",
    "    \n",
    "    # save scalers\n",
    "    scalers[pair_idx] = scaler\n",
    "    if printer:\n",
    "        printer = False\n",
    "        pass\n",
    "    \n",
    "    pair_idx += 1\n",
    "\n",
    "print(\"\\nDONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full X dataset\", x_train.shape)\n",
    "print(\"Full Y dataset\", y_train.shape)\n",
    "train_test_split_ratio = 0.8\n",
    "trainX = x_train.reshape(n_pairs, n_samples, n_timesteps, n_features)\n",
    "trainY = y_train.reshape(n_pairs, n_samples, n_features)\n",
    "\n",
    "split_index = int(n_samples*train_test_split_ratio)\n",
    "testX = trainX[:,split_index:]\n",
    "trainX = trainX[:,:split_index]\n",
    "testY = trainY[:,split_index:]\n",
    "trainY = trainY[:,:split_index]\n",
    "\n",
    "# train_x_scalers = []\n",
    "# test_scalers = []\n",
    "# for a in range(len(trainX[0])):\n",
    "#     scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "#     trainX[0][a] = scaler_x.fit_transform(trainX[0][a])\n",
    "#     trainY[0][a] = scaler_x.transform(trainY[0][a].reshape(-1,1))\n",
    "# for a in range(len(testX[0])):\n",
    "#     scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "#     testX[0][a] = scaler_x.fit_transform(testX[0][a])\n",
    "    \n",
    "\n",
    "print(\"trainX \", trainX.shape)\n",
    "print(\"trainY \", trainY.shape)\n",
    "print(\"testX \", testX.shape)\n",
    "print(\"testY \", testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DEFINE AND TRAIN PREDICTIVE LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=((trainX.shape[2], trainX.shape[3]))))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss=MAPE, optimizer='adam')\n",
    "print(model.summary())\n",
    "model.fit(trainX[0], trainY[0], epochs=5, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(testX[0])\n",
    "\n",
    "print(\"True     :\", np.exp(testY[0][0]))\n",
    "print(\"Predicted:\", np.exp(predicted[0]))\n",
    "print(testY.shape)\n",
    "print(predicted.shape)\n",
    "\n",
    "plt.plot(np.exp(testY)[0,:,0], color = 'red', label = 'Stock Price')\n",
    "plt.plot(np.exp(predicted)[:,0], color = 'green', label = 'Predicted Stock Price')\n",
    "# plt.gca().set_ylim(bottom=0)\n",
    "plt.legend()\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "mean_absolute_percentage_error(np.exp(testY[0,:,0]), np.exp(predicted[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.exp(testY)[0,:,1], color = 'red', label = 'Stock Price')\n",
    "plt.plot(np.exp(predicted)[:,1], color = 'green', label = 'Predicted Stock Price')\n",
    "# plt.gca().set_ylim(bottom=0)\n",
    "plt.legend()\n",
    "mean_absolute_percentage_error(np.exp(testY[0,:,1]), np.exp(predicted[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # TEST MODEL PREFORMANCE\n",
    "# for pair in range(len(filtered_pairs)):\n",
    "#     # make predictions\n",
    "#     trainPredict = model.predict(trainX[pair])\n",
    "#     testPredict = model.predict(testX[pair])\n",
    "    \n",
    "# #     print(testX[pair])\n",
    "#     if np.sum(np.isnan(testX[pair])):\n",
    "#         print(\"found NAN\")\n",
    "#     print(len(trainX[pair,:,0,1]))\n",
    "#     print(trainX[pair].shape)\n",
    "#     print(trainPredict.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # invert predictions (de-scale)\n",
    "#     scaler = scalers[pair]\n",
    "# #     trainPredict = scaler.inverse_transform(trainPredict)\n",
    "# #     trainY[pair] = scaler.inverse_transform([trainY[pair]])\n",
    "# #     testPredict = scaler.inverse_transform(testPredict)\n",
    "# #     testY[pair] = scaler.inverse_transform([testY[pair]])\n",
    "#     # calculate root mean squared error\n",
    "#     trainScore = math.sqrt(mean_squared_error(np.exp(trainY[pair]), np.exp(trainPredict)))\n",
    "#     print('Train Score: %.2f RMSE' % (trainScore))\n",
    "#     testScore = math.sqrt(mean_squared_error(np.exp(testY[pair]), np.exp(testPredict)))\n",
    "#     print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING INVESTING AGENT POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST AGENT PERFORMANCE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
